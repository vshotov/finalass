# -*- coding: utf-8 -*-
# ä½¿ç”¨å‰10æ¡æ¨æ–‡è¿›è¡Œ LDA ä¸»é¢˜å»ºæ¨¡ï¼ˆæ—  nltkï¼Œå»é™¤ç½‘å€ï¼‰

import re
import gensim
import pyLDAvis.gensim_models
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from gensim import corpora, models
from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric, strip_short, remove_stopwords

import tempfile
import os
os.environ['JOBLIB_TEMP_FOLDER'] = tempfile.mkdtemp(dir='D:\\temp')


# âœ… è®¾ç½®å­—ä½“ï¼Œé¿å… matplotlib ä¸­æ–‡ä¹±ç ï¼ˆå¦‚æ—  SimHei å¯æ›¿æ¢ä¸º Microsoft YaHeiï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# âœ… æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼ˆæ—  nltkï¼Œå»é™¤ç½‘å€ï¼‰
CUSTOM_FILTERS = [lambda x: x.lower(), strip_punctuation, strip_numeric, remove_stopwords, strip_short]
def clean_text(text):
    text = re.sub(r"http\S+", "", text)  # å»é™¤ç½‘å€
    return text

def preprocess(texts):
    return [preprocess_string(clean_text(doc), CUSTOM_FILTERS) for doc in texts]

# âœ… å‰10æ¡æ¨æ–‡ï¼ˆæ•°æ®ç›´æ¥å†™å…¥ï¼‰
data = [
    "Don't need feds to solve the #bostonbombing when we have #4chan!! http://t.co/eXQTPZqqbG",
    "PIC: Comparison of #Boston suspect Sunil Tripathi's FBI-released images/video and his MISSING poster http://t.co/EhV3ODxrJf You decide.",
    "I'm not completely convinced that it's this Sunil Tripathi fellowâ€”http://t.co/ZjWqOsRfjB",
    "Brutal lo que se puede conseguir en colaboraciÃ³n. #4Chan analizando fotos de la maratÃ³n de #Boston atando cabos... http://t.co/4eq43HFicK",
    "4chan and the bombing. just throwing it out there: http://t.co/dIySO7lXQm http://t.co/NxBi4tW8bQ",
    "4chan thinks they found pictures of the bomber  -  http://t.co/0y3w24l2q8",
    "Ola ke ase, investigando las bombas de Boston o ke ase? #4chan http://t.co/7A4IAbVmM0",
    "4chan ThinkTank - Imgur http://t.co/hQt2fhxE48",
    "@DLoesch have you seen this? Bomber #2  looks like missing student Sunil Tripathi.  http://t.co/1phQpB0aMT  http://t.co/X9pqMtnoW5",
    "da 4chan think tank BOSTON  http://t.co/0ZbKMA5z0D  #photos #suspects #bombing #marathon"
]

# âœ… å¤„ç†æ–‡æœ¬
cleaned_data = preprocess(data)

# âœ… æ„å»ºå­—å…¸ä¸è¯­æ–™åº“
dictionary = corpora.Dictionary(cleaned_data)
corpus = [dictionary.doc2bow(text) for text in cleaned_data]

# âœ… è®­ç»ƒ LDA æ¨¡å‹ï¼ˆ3ä¸ªä¸»é¢˜ï¼‰
lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)

# âœ… æ‰“å°æ¯ä¸ªä¸»é¢˜å…³é”®è¯
print("\nğŸ” æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯ï¼š")
for idx, topic in lda_model.print_topics(num_words=5):
    print(f"ä¸»é¢˜ {idx}: {topic}")

# âœ… ç”Ÿæˆè¯äº‘å›¾
for i in range(3):
    plt.figure()
    word_freqs = dict(lda_model.show_topic(i, topn=30))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"ä¸»é¢˜ {i} è¯äº‘")
    plt.tight_layout()
    plt.savefig(f"topic_{i}_wordcloud.png")

# âœ… ç”Ÿæˆ pyLDAvis å¯è§†åŒ–é¡µé¢
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.save_html(vis, 'lda_vis.html')
print("\nâœ… å¯è§†åŒ–å·²ä¿å­˜ä¸º lda_vis.htmlï¼Œè¯·ç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹ã€‚")

import joblib

joblib.dump(lda_model, "lda_model.joblib")
joblib.dump(dictionary, "lda_dictionary.joblib")

